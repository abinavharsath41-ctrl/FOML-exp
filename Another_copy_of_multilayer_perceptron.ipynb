{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abinavharsath41-ctrl/FOML-exp/blob/main/Another_copy_of_multilayer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sABLYrlhdzmc",
        "outputId": "946e7621-3045-45cc-a7eb-c9181368ad5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Banknote Authentication dataset...\n",
            "Data loaded successfully.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "   variance_of_wavelet  skewness_of_wavelet  curtosis_of_wavelet  \\\n",
            "0              3.62160               8.6661              -2.8073   \n",
            "1              4.54590               8.1674              -2.4586   \n",
            "2              3.86600              -2.6383               1.9242   \n",
            "3              3.45660               9.5228              -4.0112   \n",
            "4              0.32924              -4.4552               4.5718   \n",
            "\n",
            "   entropy_of_image  class  \n",
            "0          -0.44699      0  \n",
            "1          -1.46210      0  \n",
            "2           0.10645      0  \n",
            "3          -3.59440      0  \n",
            "4          -0.98880      0  \n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1372 entries, 0 to 1371\n",
            "Data columns (total 5 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   variance_of_wavelet  1372 non-null   float64\n",
            " 1   skewness_of_wavelet  1372 non-null   float64\n",
            " 2   curtosis_of_wavelet  1372 non-null   float64\n",
            " 3   entropy_of_image     1372 non-null   float64\n",
            " 4   class                1372 non-null   int64  \n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 53.7 KB\n",
            "\n",
            "Data split into training (960 samples) and testing (412 samples).\n",
            "\n",
            "--- Running for Activation: tanh ---\n",
            "Training model (Architecture: (10, 10), Max Iter: 500)...\n",
            "\n",
            "Evaluation Metrics:\n",
            "  Confusion Matrix:\n",
            "[[229   0]\n",
            " [  1 182]]\n",
            "  TN=229, FP=0, FN=1, TP=182\n",
            "  Accuracy: 0.9976\n",
            "  Precision (Class 0, Class 1): [0.99565217 1.        ]\n",
            "  Recall    (Class 0, Class 1): [1.         0.99453552]\n",
            "  F1-Score  (Class 0, Class 1): [0.99782135 0.99726027]\n",
            "\n",
            "--- Running for Activation: logistic ---\n",
            "Training model (Architecture: (10, 10), Max Iter: 500)...\n",
            "\n",
            "Evaluation Metrics:\n",
            "  Confusion Matrix:\n",
            "[[229   0]\n",
            " [183   0]]\n",
            "  TN=229, FP=0, FN=183, TP=0\n",
            "  Accuracy: 0.5558\n",
            "  Precision (Class 0, Class 1): [0.55582524 0.        ]\n",
            "  Recall    (Class 0, Class 1): [1. 0.]\n",
            "  F1-Score  (Class 0, Class 1): [0.71450858 0.        ]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running for Activation: identity ---\n",
            "Training model (Architecture: (10, 10), Max Iter: 500)...\n",
            "\n",
            "Evaluation Metrics:\n",
            "  Confusion Matrix:\n",
            "[[220   9]\n",
            " [ 15 168]]\n",
            "  TN=220, FP=9, FN=15, TP=168\n",
            "  Accuracy: 0.9417\n",
            "  Precision (Class 0, Class 1): [0.93617021 0.94915254]\n",
            "  Recall    (Class 0, Class 1): [0.96069869 0.91803279]\n",
            "  F1-Score  (Class 0, Class 1): [0.94827586 0.93333333]\n"
          ]
        }
      ],
      "source": [
        " # Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# --- Configuration based on Exercise 5 ---\n",
        "RANDOM_SEED = 42\n",
        "MAX_ITER = 500\n",
        "VALIDATION_FRACTION = 0.1\n",
        "ARCHITECTURES = [(10, 10)] # Two hidden layers, 10 neurons each\n",
        "SOLVER = 'adam'\n",
        "\n",
        "\n",
        "def load_banknote_data():\n",
        "    \"\"\"\n",
        "    Downloads the UCI Banknote Authentication dataset from the UCI repository.\n",
        "    \"\"\"\n",
        "    print(\"Loading Banknote Authentication dataset...\")\n",
        "    # Direct link to the raw data file (common for UCI datasets)\n",
        "    data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\"\n",
        "    try:\n",
        "        response = requests.get(data_url, timeout=10)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses\n",
        "\n",
        "        # Define column names based on the UCI documentation\n",
        "        col_names = [\n",
        "            'variance_of_wavelet',\n",
        "            'skewness_of_wavelet',\n",
        "            'curtosis_of_wavelet',\n",
        "            'entropy_of_image',\n",
        "            'class' # The target variable\n",
        "        ]\n",
        "\n",
        "        # Load data directly from the text content\n",
        "        data = pd.read_csv(StringIO(response.text), header=None, names=col_names)\n",
        "        print(\"Data loaded successfully.\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading data. Please check the URL or your internet connection: {e}\")\n",
        "        # Fallback: Create a dummy DataFrame to prevent script crash\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def build_train_and_evaluate_mlp(X_train, X_test, y_train, y_test, activation_func, architecture, max_iter):\n",
        "    \"\"\"\n",
        "    Builds, trains, and evaluates an MLPClassifier with given parameters.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running for Activation: {activation_func} ---\")\n",
        "\n",
        "    # Step 6: Build MLP model\n",
        "    # Note: validation_fraction is handled internally by scikit-learn's MLPClassifier\n",
        "    mlp = MLPClassifier(\n",
        "        hidden_layer_sizes=architecture,\n",
        "        activation=activation_func, # Step 6c: Activation to test\n",
        "        solver=SOLVER,             # Step 6b: Optimizer\n",
        "        alpha=0.0001,              # L2 penalty (regularization term parameter)\n",
        "        max_iter=max_iter,         # Step 6d: Learning controls\n",
        "        random_state=RANDOM_SEED,  # Step 6d: Random seed\n",
        "        early_stopping=True,       # Step 6d: Early stopping enabled\n",
        "        validation_fraction=VALIDATION_FRACTION, # Step 6d: Validation fraction\n",
        "        n_iter_no_change=10,       # Stop if loss doesn't improve for 10 consecutive epochs\n",
        "        verbose=False              # Set to True to see training progress\n",
        "    )\n",
        "\n",
        "    # Step 7: Train the model\n",
        "    print(f\"Training model (Architecture: {architecture}, Max Iter: {max_iter})...\")\n",
        "\n",
        "    # The fit method records the training progress in the 'loss_curve_' and 'validation_scores_' attributes\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    # Step 8: Do the testing process and compute metrics\n",
        "    y_pred = mlp.predict(X_test)\n",
        "\n",
        "    # Compute Confusion Matrix (TN, FP, FN, TP)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    # cm is [[TN, FP], [FN, TP]] for a binary classification\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "    # Compute accuracy, precision, recall, f1-score for each class\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    # precision_recall_fscore_support returns (precision, recall, f1, support)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
        "\n",
        "    print(\"\\nEvaluation Metrics:\")\n",
        "    print(f\"  Confusion Matrix:\\n{cm}\")\n",
        "    print(f\"  TN={TN}, FP={FP}, FN={FN}, TP={TP}\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision (Class 0, Class 1): {precision}\")\n",
        "    print(f\"  Recall    (Class 0, Class 1): {recall}\")\n",
        "    print(f\"  F1-Score  (Class 0, Class 1): {f1}\")\n",
        "\n",
        "    return mlp\n",
        "\n",
        "def plot_training_curves(mlp, activation_func):\n",
        "    \"\"\"\n",
        "    Step 9: Plot training loss and validation accuracy per epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    if len(mlp.loss_curve_) > 0:\n",
        "        epochs = range(1, len(mlp.loss_curve_) + 1)\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, mlp.loss_curve_, label='Training Loss')\n",
        "        plt.title(f'Training Loss per Epoch ({activation_func})')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot Validation Accuracy (Note: validation_scores_ is only available when early_stopping=True)\n",
        "        if len(mlp.validation_scores_) > 0:\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(epochs[:len(mlp.validation_scores_)], mlp.validation_scores_, label='Validation Accuracy', color='orange')\n",
        "            plt.title(f'Validation Accuracy per Epoch ({activation_func})')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.grid(True)\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Could not retrieve loss curve data.\")\n",
        "\n",
        "def main():\n",
        "    # Load the data\n",
        "    data = load_banknote_data()\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"Cannot proceed without data.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: View first few rows of the dataset\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())\n",
        "    print(\"\\nDataset Info:\")\n",
        "    data.info()\n",
        "\n",
        "    # Step 4: Separate features (X) and target (y)\n",
        "    X = data.iloc[:, :-1]  # All columns except the last one (features)\n",
        "    y = data.iloc[:, -1]   # The last column (target)\n",
        "\n",
        "    # Step 5: Split the dataset for training and testing\n",
        "    # Using stratify=y ensures that the train/test split has the same proportion of class labels\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y\n",
        "    )\n",
        "    print(f\"\\nData split into training ({len(X_train)} samples) and testing ({len(X_test)} samples).\")\n",
        "\n",
        "    # Step 10: Repeat steps 6 to 9 for different activation functions\n",
        "    # Tanh, Logistic (Sigmoid), Identity\n",
        "    activation_functions = ['tanh', 'logistic', 'identity']\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for activation in activation_functions:\n",
        "        # Step 6-9 execution\n",
        "        mlp_model = build_train_and_evaluate_mlp(\n",
        "            X_train, X_test, y_train, y_test, activation, ARCHITECTURES[0], MAX_ITER\n",
        "        )\n",
        "        plot_training_curves(mlp_model, activation)\n",
        "        results[activation] = mlp_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure matplotlib is set to non-interactive mode if running in a headless environment\n",
        "    try:\n",
        "        plt.switch_backend('Agg')\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
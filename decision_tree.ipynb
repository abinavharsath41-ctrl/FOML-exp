{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abinavharsath41-ctrl/FOML-exp/blob/main/decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5p4LGkSif7R",
        "outputId": "50b7b1d6-559f-4b03-ae69-f1460c646f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Iris Dataset Head ---\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "Feature Names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Target Names: ['Setosa', 'Versicolor', 'Virginica']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Full Decision Tree (All 4 Features) ---\n",
            "Full Tree Depth: 4\n",
            "\n",
            "Displaying all plots (decision boundaries and full tree)...\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from itertools import combinations\n",
        "import pandas as pd\n",
        "\n",
        "# --- Configuration based on Exercise ---\n",
        "RANDOM_STATE = 42\n",
        "PLOT_COLORS = \"ryb\"\n",
        "PLOT_STEP = 0.02\n",
        "# The target names are needed for the legend/tree\n",
        "TARGET_NAMES = ['Setosa', 'Versicolor', 'Virginica']\n",
        "# ----------------------------------------\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Step 2: Load the dataset (Iris) and Step 3: View first few rows.\"\"\"\n",
        "\n",
        "    # Load the Iris dataset\n",
        "    iris = load_iris(as_frame=True)\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    feature_names = iris.feature_names\n",
        "\n",
        "    # Step 3: View first few rows of the dataset\n",
        "    print(\"--- Iris Dataset Head ---\")\n",
        "    print(pd.concat([X, y], axis=1).head())\n",
        "    print(\"\\nFeature Names:\", feature_names)\n",
        "    print(\"Target Names:\", TARGET_NAMES)\n",
        "\n",
        "    return X, y, feature_names\n",
        "\n",
        "def plot_decision_boundary(X_pair, y, clf, title, feature_names):\n",
        "    \"\"\"\n",
        "    Step 5: Creates a grid, predicts, plots the background, and plots the data points.\n",
        "\n",
        "    FIXED: The indexing within plt.scatter is corrected to use single-dimension indexing\n",
        "    on the Series (feature columns).\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the two columns of the DataFrame X_pair as pandas Series\n",
        "    feature1 = X_pair.iloc[:, 0]\n",
        "    feature2 = X_pair.iloc[:, 1]\n",
        "\n",
        "    # Define the axes limits based on the features\n",
        "    x_min, x_max = feature1.min() - 1, feature1.max() + 1\n",
        "    y_min, y_max = feature2.min() - 1, feature2.max() + 1\n",
        "\n",
        "    # Create a grid of x and y values to cover all feature points.\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.arange(x_min, x_max, PLOT_STEP),\n",
        "        np.arange(y_min, y_max, PLOT_STEP)\n",
        "    )\n",
        "\n",
        "    # Predict the class for each grid point\n",
        "    # We use clf.predict on the flattened meshgrid coordinates\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # Color the background based on predicted class using contourf()\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.8)\n",
        "\n",
        "    # Plot the real data points (Iris samples) on top of the background\n",
        "    for i, color in zip(np.unique(y), PLOT_COLORS):\n",
        "        # Select data points belonging to the current class\n",
        "        idx = np.where(y == i)\n",
        "\n",
        "        # --- THE INDEXING FIX IS HERE ---\n",
        "        # The scatter plot takes the values from the feature Series, indexed by the row indices (idx[0]).\n",
        "        # The original code X_pair.iloc[idx, 0] caused \"Too many indexers\" because X_pair.iloc[idx]\n",
        "        # is a two-dimensional call on a Series derived from a DataFrame, which is ambiguous/incorrect.\n",
        "        plt.scatter(\n",
        "            feature1.iloc[idx[0]],  # Corrected: Use iloc on the Series, index with the row indices array idx[0]\n",
        "            feature2.iloc[idx[0]],  # Corrected: Use iloc on the Series, index with the row indices array idx[0]\n",
        "            c=color,\n",
        "            label=TARGET_NAMES[i],\n",
        "            edgecolor='black',\n",
        "            s=20\n",
        "        )\n",
        "\n",
        "    # Label the axes and add a legend\n",
        "    plt.xlabel(feature_names[0])\n",
        "    plt.ylabel(feature_names[1])\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "\n",
        "def train_and_plot_2d_boundaries(X, y, feature_names):\n",
        "    \"\"\"Handles the iteration over feature pairs (Step 5).\"\"\"\n",
        "\n",
        "    # pairs -> makes all combinations of 2 features: (0, 1), (0, 2), etc.\n",
        "    feature_indices = range(X.shape[1])\n",
        "    feature_pairs = list(combinations(feature_indices, 2))\n",
        "\n",
        "    for pair_indices in feature_pairs:\n",
        "\n",
        "        # Take only those two features -> X_pair\n",
        "        X_pair_data = X.iloc[:, list(pair_indices)]\n",
        "\n",
        "        # Train the Decision Tree Model -> clf.fit(X_pair, y)\n",
        "        clf = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=5)\n",
        "        clf.fit(X_pair_data, y)\n",
        "\n",
        "        # Get the names of the two features for the plot title and labels\n",
        "        pair_feature_names = [feature_names[i] for i in pair_indices]\n",
        "        title = f\"Decision Boundary: {pair_feature_names[0]} vs {pair_feature_names[1]}\"\n",
        "\n",
        "        # Call the plotting function\n",
        "        plot_decision_boundary(X_pair_data, y.values, clf, title, pair_feature_names)\n",
        "\n",
        "\n",
        "def train_and_plot_full_tree(X, y, feature_names):\n",
        "    \"\"\"Step 6: Train a tree using all features and display the tree diagram.\"\"\"\n",
        "\n",
        "    print(\"\\n--- Training Full Decision Tree (All 4 Features) ---\")\n",
        "\n",
        "    # Train one Decision Tree using all 4 features (not just pairs)\n",
        "    full_clf = DecisionTreeClassifier(random_state=RANDOM_STATE, max_depth=4)\n",
        "    full_clf.fit(X, y)\n",
        "\n",
        "    print(f\"Full Tree Depth: {full_clf.get_depth()}\")\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # plot_tree() is used to display the tree diagram.\n",
        "    plot_tree(\n",
        "        full_clf,\n",
        "        feature_names=feature_names,\n",
        "        class_names=TARGET_NAMES,\n",
        "        filled=True,              # Color nodes by class\n",
        "        rounded=True,             # Round the boxes\n",
        "        proportion=True,          # Show class proportions\n",
        "        fontsize=8\n",
        "    )\n",
        "    plt.title(\"Full Decision Tree Trained on All Features\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    X, y, feature_names = load_data()\n",
        "\n",
        "    # Train and plot decision boundaries for all 2D pairs (Step 5)\n",
        "    train_and_plot_2d_boundaries(X, y, feature_names)\n",
        "\n",
        "    # Train and plot the full tree (Step 6)\n",
        "    train_and_plot_full_tree(X, y, feature_names)\n",
        "\n",
        "    # Step 7: Show all plots\n",
        "    print(\"\\nDisplaying all plots (decision boundaries and full tree)...\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Standard check for non-interactive environments\n",
        "    try:\n",
        "        plt.switch_backend('TkAgg') # Use a standard interactive backend\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    main()"
      ]
    }
  ]
}